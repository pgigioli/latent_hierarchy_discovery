{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "stop_words = list(stop_words.ENGLISH_STOP_WORDS) + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'n_features' : 10000,\n",
    "    'hidden_dim' : 128,\n",
    "    'latent_dims' : [10, 10, 10],\n",
    "    'batch_size' : 16,\n",
    "    'lr' : 0.001,\n",
    "    'dropout' : 0.5,\n",
    "    'n_epochs' : 25,\n",
    "    'clf_loss_weight' : 1.0,\n",
    "    'latent_loss_weight' : 1.0,\n",
    "    'recon_loss_weight' : 1.0,\n",
    "    'display_interval' : 100,\n",
    "    'val_interval' : 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=hparams['n_features'], stop_words=stop_words)\n",
    "train_features = tfidf.fit_transform(train.data)\n",
    "test_features = tfidf.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def straight_through_estimator(logits):\n",
    "    argmax = torch.eq(logits, logits.max(-1, keepdim=True).values).to(logits.dtype)\n",
    "    return (argmax - logits).detach() + logits\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0, eps=1e-20):\n",
    "    u = torch.rand(logits.size(), dtype=logits.dtype, device=logits.device)\n",
    "    g = -torch.log(-torch.log(u + eps) + eps)\n",
    "    return F.softmax((logits + g) / temperature, dim=-1)\n",
    "\n",
    "class CategoricalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, categorical_dim, output_dim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if output_dim == None:\n",
    "            output_dim = input_dim\n",
    "            \n",
    "        self.dense_in = nn.Linear(input_dim, categorical_dim, bias=True)\n",
    "        self.dense_out = nn.Linear(input_dim+categorical_dim, output_dim, bias=True)\n",
    "        \n",
    "    def forward(self, inputs, straight_through=True, sample=False, temperature=1.0, return_logits=False):\n",
    "        logits = self.dense_in(inputs)\n",
    "        \n",
    "        if sample:\n",
    "            dist = gumbel_softmax(logits, temperature=temperature)\n",
    "        else:\n",
    "            dist = F.softmax(logits, dim=-1)\n",
    "            \n",
    "        if straight_through:\n",
    "            dist = straight_through_estimator(dist)\n",
    "            \n",
    "        h = torch.tanh(self.dense_out(torch.cat([inputs, dist], dim=-1)))\n",
    "        \n",
    "        if return_logits:\n",
    "            return h, dist, logits\n",
    "        else:\n",
    "            return h, dist\n",
    "    \n",
    "class HLGC(nn.Module):\n",
    "    def __init__(self, n_classes, input_dim, categorical_dims, hidden_dim=128, dropout_rate=0.5, batch_size=16,\n",
    "                 n_epochs=25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.input_dim = input_dim\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # classifier\n",
    "        self.input_dense = nn.Linear(self.input_dim, self.hidden_dim, bias=True)\n",
    "        \n",
    "        self.categorical_layers = nn.ModuleList([\n",
    "            CategoricalLayer(self.hidden_dim, dim) for dim in self.categorical_dims\n",
    "        ])\n",
    "        \n",
    "        self.global_dense = nn.Linear(sum(self.categorical_dims), self.hidden_dim, bias=True)\n",
    "        self.out_dense = nn.Linear(self.hidden_dim, self.n_classes, bias=True)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # generator\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_dim+dim, self.hidden_dim, bias=True) for dim in self.categorical_dims\n",
    "        ])\n",
    "        self.encoder_out = nn.Linear(self.hidden_dim, self.n_classes, bias=True) \n",
    "        \n",
    "        self.decoder_in = nn.Linear(self.n_classes, self.hidden_dim, bias=True)\n",
    "        self.decoder = nn.ModuleList([\n",
    "            CategoricalLayer(self.hidden_dim, dim) for dim in self.categorical_dims\n",
    "        ])\n",
    "        \n",
    "    def encode(self, dists):\n",
    "        h = torch.zeros(dists[0].size(0), self.hidden_dim, device=dists[0].device) \n",
    "        \n",
    "        for dist, layer in zip(dists, self.encoder):\n",
    "            h = torch.tanh(layer(torch.cat([h, dist], dim=-1))) \n",
    "        z = self.encoder_out(h)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z_sample, straight_through=True, temperature=1.0): \n",
    "        h = torch.tanh(self.decoder_in(z_sample))\n",
    "        \n",
    "        gen_states, gen_logits = [], []\n",
    "        for layer in self.decoder:\n",
    "            h, dist, logits = layer(\n",
    "                h, straight_through=straight_through, temperature=temperature, sample=True, \n",
    "                return_logits=True\n",
    "            )\n",
    "            gen_states.append(dist)\n",
    "            gen_logits.append(logits)\n",
    "        return gen_states, gen_logits\n",
    "    \n",
    "    def classify(self, inputs, return_states=False):\n",
    "        h = torch.tanh(self.input_dense(inputs))\n",
    "        self.dropout(h)\n",
    "        \n",
    "        states = []\n",
    "        for layer in self.categorical_layers:\n",
    "            h, dist = layer(h, straight_through=True, sample=False)\n",
    "            self.dropout(h)\n",
    "            states.append(dist)\n",
    "            \n",
    "        h = torch.tanh(self.global_dense(torch.cat(states, dim=-1)))\n",
    "        logits = self.out_dense(h)\n",
    "        \n",
    "        if return_states:\n",
    "            return logits, states\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def forward(self, inputs, temperature=1.0):\n",
    "        # classifier\n",
    "        clf_logits, clf_states = self.classify(inputs, return_states=True)\n",
    "        \n",
    "        # generator\n",
    "        z = self.encode([x.detach() for x in clf_states])\n",
    "        z_sample = straight_through_estimator(gumbel_softmax(z, temperature=temperature))\n",
    "        gen_states, gen_logits = self.generate(z_sample, straight_through=True, temperature=temperature)\n",
    "        return clf_logits, clf_states, gen_logits, gen_states, z\n",
    "    \n",
    "    def fit(self, train_features, train_targets, val_features=None, val_targets=None):\n",
    "        train_loader = DataLoader(\n",
    "            list(zip(train_features, train_targets)), \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for batch in train_loader:\n",
    "                features, targets = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HLGC(20, hparams['n_features'], hparams['latent_dims'], hidden_dim=hparams['hidden_dim'], \n",
    "             dropout_rate=hparams['dropout'], batch_size=hparams['batch_size'], n_epochs=hparams['n_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = Counter(train.target)\n",
    "class_weights = torch.tensor([class_counts[i] for i in range(len(class_counts))], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    list(zip(train_features.toarray().astype(np.float32), train.target)), \n",
    "    batch_size=hparams['batch_size'], \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()), betas=(0.9, 0.98),\n",
    "    eps=1e-09,\n",
    "    lr=hparams['lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(hparams['n_epochs']):\n",
    "    itr = 0\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        itr += 1\n",
    "\n",
    "        features, targets = batch\n",
    "        clf_logits, clf_states, gen_logits, gen_states, z = model(features)\n",
    "\n",
    "        clf_loss = nn.CrossEntropyLoss(weight=class_weights)(clf_logits, targets)\n",
    "\n",
    "        recon_loss = 0.0\n",
    "        for clf_state, gen_logit in zip(clf_states, gen_logits):\n",
    "            recon_loss += nn.CrossEntropyLoss()(gen_logit, clf_state.argmax(-1))\n",
    "            recon_loss /= len(clf_state)\n",
    "\n",
    "        latent_loss = nn.CrossEntropyLoss(weight=class_weights)(z, clf_logits.argmax(-1))    \n",
    "\n",
    "        loss = (\n",
    "            hparams['clf_loss_weight']*clf_loss + \n",
    "            hparams['recon_loss_weight']*recon_loss +\n",
    "            hparams['latent_loss_weight']*latent_loss\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if itr == 1 or itr % hparams['display_interval'] == 0:\n",
    "            clf_acc = (targets == clf_logits.argmax(-1)).to(float).mean()\n",
    "            \n",
    "            recon_acc = 0.0\n",
    "            for clf_state, gen_state in zip(clf_states, gen_states):\n",
    "                recon_acc += (clf_state.argmax(-1) == gen_state.argmax(-1)).to(float).mean()\n",
    "            recon_acc /= len(clf_states)\n",
    "            \n",
    "            log_string = '[{}, {:5d}] loss - (total : {:3f}, clf : {:3f}, latent : {:3f}, recon : {:3f}), \\\n",
    "acc - (clf : {:3f}, recon : {:3f})'.format(epoch, itr, loss.item(), clf_loss.item(), latent_loss.item(), \n",
    "                                           recon_loss.item(), clf_acc.item(), recon_acc.item())\n",
    "            print(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
